{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import PegasusForConditionalGeneration, PegasusTokenizer\nfrom peft import get_peft_model, LoraConfig, TaskType\nimport torch\nmodel_name = \"google/pegasus-xsum\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#pip install --upgrade transformers peft accelerate bitsandbytes\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import PegasusForConditionalGeneration, PegasusTokenizer\nfrom transformers import BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16\n)\n\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\n\nmodel = PegasusForConditionalGeneration.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map={\"\": 0},\n    torch_dtype=torch.float16\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\nmodel = prepare_model_for_kbit_training(model)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Model device: {next(model.parameters()).device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"knkarthick/samsum\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ndata=dataset[\"train\"].select(range(5000))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\ndef preprocess_function(examples):\n    inputs = examples[\"dialogue\"]\n    targets = examples[\"summary\"]\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(targets, max_length=64, truncation=True, padding=\"max_length\")\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_dataset = data.map(preprocess_function)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\nfrom peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"],  # adjust based on actual layer names\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def data_collator(batch):\n    input_ids = torch.stack([torch.tensor(example['input_ids']) for example in batch])\n    attention_mask = torch.stack([torch.tensor(example['attention_mask']) for example in batch])\n    labels = torch.stack([torch.tensor(example['labels']) for example in batch])\n    \n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir=\"./pegasus-lora\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    learning_rate=2e-4,\n    num_train_epochs=3,\n    #evaluation_strategy=\"epoch\",\n    #save_strategy=\"epoch\",\n    logging_dir=\"None\",\n    fp16=True,  # if you're using mixed precision\n    gradient_checkpointing=True\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=data_collator\n)\n\ntrainer.train()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n\nPegasusTokenizer.from_pretrained(\"google/pegasus-xsum\").save_pretrained(\"google-pegasus-xsum\")\nPegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\").save_pretrained(\"google-pegasus-xsum\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}